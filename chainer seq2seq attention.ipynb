{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Model + Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from chainer import Chain, Variable, cuda, optimizer, optimizers, serializers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.5.2\n",
      "\n",
      "numpy 1.14.2\n",
      "chainer 3.5.0\n",
      "mecab-python3 0.7\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import get_distribution\n",
    "import platform\n",
    "print(\"python\", platform.python_version())\n",
    "print(\"\")\n",
    "libs = [\"numpy\", \"chainer\", \"mecab-python3\"]\n",
    "for lib in libs:\n",
    "    version = get_distribution(lib).version\n",
    "    print(lib, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUのセット\n",
    "FLAG_GPU = True # GPUを使用するかどうか\n",
    "if FLAG_GPU: # numpyかcuda.cupyか\n",
    "    xp = cuda.cupy\n",
    "    cuda.get_device(0).use()\n",
    "else:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ変換クラスの定義\n",
    "class DataConverter:\n",
    "    def __init__(self, batch_col_size):\n",
    "        # クラスの初期化\n",
    "        # :param batch_col_size: 学習時のミニバッチ単語数サイズ\n",
    "        self.mecab = MeCab.Tagger() # 形態素解析器\n",
    "        self.vocab = {\"<eos>\":0, \"<unknown>\": 1} # 単語辞書\n",
    "        self.batch_col_size = batch_col_size\n",
    "        \n",
    "    def load(self, data):\n",
    "        # 学習時に、教師データを読み込んでミニバッチサイズに対応したNumpy配列に変換する\n",
    "        # :param data: 対話データ\n",
    "        # 単語辞書の登録\n",
    "        self.vocab = {\"<eos>\":0, \"<unknown>\": 1} # 単語辞書を初期化\n",
    "        for d in data:\n",
    "            sentences = [d[0][0], d[1][0]] # 入力文、返答文\n",
    "            for sentence in sentences:\n",
    "                sentence_words = self.sentence2words(sentence) # 文章を単語に分解する\n",
    "                for word in sentence_words:\n",
    "                    if word not in self.vocab:\n",
    "                        self.vocab[word] = len(self.vocab)\n",
    "        # 教師データのID化と整理\n",
    "        queries, responses = [], []\n",
    "        for d in data:\n",
    "            query, response = d[0][0], d[1][0] #  エンコード文、デコード文\n",
    "            queries.append(self.sentence2ids(sentence=query, train=True, sentence_type=\"query\"))\n",
    "            responses.append(self.sentence2ids(sentence=response, train=True, sentence_type=\"response\"))\n",
    "        self.train_queries = xp.vstack(queries)\n",
    "        self.train_responses = xp.vstack(responses)\n",
    "    \n",
    "    def sentence2words(self, sentence):\n",
    "        # 文章を単語の配列にして返却する\n",
    "        # :param sentence: 文章文字列\n",
    "        sentence_words = []\n",
    "        for m in self.mecab.parse(sentence).split(\"\\n\"): # 形態素解析で単語に分解する\n",
    "            w = m.split(\"\\t\")[0].lower() # 単語\n",
    "            if len(w) == 0 or w == \"eos\": # 不正文字、EOSは省略\n",
    "                continue\n",
    "            sentence_words.append(w)\n",
    "        sentence_words.append(\"<eos>\") # 最後にvocabに登録している<eos>を代入する\n",
    "        return sentence_words\n",
    "\n",
    "    def sentence2ids(self, sentence, train=True, sentence_type=\"query\"):\n",
    "        # 文章を単語IDのNumpy配列に変換して返却する\n",
    "        # :param sentence: 文章文字列\n",
    "        # :param train: 学習用かどうか\n",
    "        # :sentence_type: 学習用でミニバッチ対応のためのサイズ補填方向をクエリー・レスポンスで変更するため\"query\"or\"response\"を指定　\n",
    "        # :return: 単語IDのNumpy配列\n",
    "        ids = [] # 単語IDに変換して格納する配列\n",
    "        sentence_words = self.sentence2words(sentence) # 文章を単語に分解する\n",
    "        for word in sentence_words:\n",
    "            if word in self.vocab: # 単語辞書に存在する単語ならば、IDに変換する\n",
    "                ids.append(self.vocab[word])\n",
    "            else: # 単語辞書に存在しない単語ならば、<unknown>に変換する\n",
    "                ids.append(self.vocab[\"<unknown>\"])\n",
    "        # 学習時は、ミニバッチ対応のため、単語数サイズを調整してNumpy変換する\n",
    "        if train:\n",
    "            if sentence_type == \"query\": # クエリーの場合は前方にミニバッチ単語数サイズになるまで-1を補填する\n",
    "                while len(ids) > self.batch_col_size: # ミニバッチ単語サイズよりも大きければ、ミニバッチ単語サイズになるまで先頭から削る\n",
    "                    ids.pop(0)\n",
    "                ids = xp.array([-1]*(self.batch_col_size-len(ids))+ids, dtype=\"int32\")\n",
    "            elif sentence_type == \"response\": # レスポンスの場合は後方にミニバッチ単語数サイズになるまで-1を補填する\n",
    "                while len(ids) > self.batch_col_size: # ミニバッチ単語サイズよりも大きければ、ミニバッチ単語サイズになるまで末尾から削る\n",
    "                    ids.pop()\n",
    "                ids = xp.array(ids+[-1]*(self.batch_col_size-len(ids)), dtype=\"int32\")\n",
    "        else: # 予測時は、そのままNumpy変換する\n",
    "            ids = xp.array([ids], dtype=\"int32\")\n",
    "        return ids\n",
    "        \n",
    "    def ids2words(self, ids):\n",
    "        # 予測時に、単語IDのNumpy配列を単語に変換して返却する\n",
    "        # :param ids: 単語IDのNumpy配列\n",
    "        # :return: 単語の配列\n",
    "        words = [] # 単語を格納する配列\n",
    "        for i in ids: # 順番に単語IDを単語辞書から参照して単語に変換する\n",
    "            words.append(list(self.vocab.keys())[list(self.vocab.values()).index(i)])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルクラスの定義\n",
    "\n",
    "# LSTMエンコーダークラス\n",
    "class LSTMEncoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        # Encoderのインスタンス化\n",
    "        # :param vocab_size: 使われる単語の種類数\n",
    "        # :param embed_size: 単語をベクトル表現した際のサイズ\n",
    "        # :param hidden_size: 隠れ層のサイズ\n",
    "        super(LSTMEncoder, self).__init__(\n",
    "            xe = L.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            eh = L.Linear(embed_size, 4 * hidden_size),\n",
    "            hh = L.Linear(hidden_size, 4 * hidden_size)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, c, h):\n",
    "        # Encoderの計算\n",
    "        # :param x: one-hotな単語\n",
    "        # :param c: 内部メモリ\n",
    "        # :param h: 隠れ層\n",
    "        # :return: 次の内部メモリ、次の隠れ層\n",
    "        e = F.tanh(self.xe(x))\n",
    "        return F.lstm(c, self.eh(e) + self.hh(h))\n",
    "\n",
    "# Attention Model + LSTMデコーダークラス\n",
    "class AttLSTMDecoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        # Attention ModelのためのDecoderのインスタンス化\n",
    "        # :param vocab_size: 語彙数\n",
    "        # :param embed_size: 単語ベクトルのサイズ\n",
    "        # :param hidden_size: 隠れ層のサイズ\n",
    "        super(AttLSTMDecoder, self).__init__(\n",
    "            ye = L.EmbedID(vocab_size, embed_size, ignore_label=-1), # 単語を単語ベクトルに変換する層\n",
    "            eh = L.Linear(embed_size, 4 * hidden_size), # 単語ベクトルを隠れ層の4倍のサイズのベクトルに変換する層\n",
    "            hh = L.Linear(hidden_size, 4 * hidden_size), # Decoderの中間ベクトルを隠れ層の4倍のサイズのベクトルに変換する層\n",
    "            fh = L.Linear(hidden_size, 4 * hidden_size), # 順向きEncoderの中間ベクトルの加重平均を隠れ層の4倍のサイズのベクトルに変換する層\n",
    "            bh = L.Linear(hidden_size, 4 * hidden_size), # 順向きEncoderの中間ベクトルの加重平均を隠れ層の4倍のサイズのベクトルに変換する層\n",
    "            he = L.Linear(hidden_size, embed_size), # 隠れ層サイズのベクトルを単語ベクトルのサイズに変換する層\n",
    "            ey = L.Linear(embed_size, vocab_size) # 単語ベクトルを語彙数サイズのベクトルに変換する層\n",
    "        )\n",
    "\n",
    "    def __call__(self, y, c, h, f, b):\n",
    "        # Decoderの計算\n",
    "        # :param y: Decoderに入力する単語\n",
    "        # :param c: 内部メモリ\n",
    "        # :param h: Decoderの中間ベクトル\n",
    "        # :param f: Attention Modelで計算された順向きEncoderの加重平均\n",
    "        # :param b: Attention Modelで計算された逆向きEncoderの加重平均\n",
    "        # :return: 語彙数サイズのベクトル、更新された内部メモリ、更新された中間ベクトル\n",
    "        e = F.tanh(self.ye(y)) # 単語を単語ベクトルに変換\n",
    "        c, h = F.lstm(c, self.eh(e) + self.hh(h) + self.fh(f) + self.bh(b)) # 単語ベクトル、Decoderの中間ベクトル、順向きEncoderのAttention、逆向きEncoderのAttentionを使ってLSTM\n",
    "        t = self.ey(F.tanh(self.he(h))) # LSTMから出力された中間ベクトルを語彙数サイズのベクトルに変換する\n",
    "        return t, c, h\n",
    "\n",
    "# Attentionモデルクラス\n",
    "class Attention(Chain):\n",
    "    def __init__(self, hidden_size):\n",
    "        # Attentionのインスタンス化\n",
    "        # :param hidden_size: 隠れ層のサイズ\n",
    "        super(Attention, self).__init__(\n",
    "            fh = L.Linear(hidden_size, hidden_size), # 順向きのEncoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層\n",
    "            bh = L.Linear(hidden_size, hidden_size), # 逆向きのEncoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層\n",
    "            hh = L.Linear(hidden_size, hidden_size), # Decoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層\n",
    "            hw = L.Linear(hidden_size, 1), # 隠れ層サイズのベクトルをスカラーに変換するための線形結合層\n",
    "        )\n",
    "        self.hidden_size = hidden_size # 隠れ層のサイズを記憶\n",
    "\n",
    "    def __call__(self, fs, bs, h):\n",
    "        # Attentionの計算\n",
    "        # :param fs: 順向きのEncoderの中間ベクトルが記録されたリスト\n",
    "        # :param bs: 逆向きのEncoderの中間ベクトルが記録されたリスト\n",
    "        # :param h: Decoderで出力された中間ベクトル\n",
    "        # :return: 順向きのEncoderの中間ベクトルの加重平均と逆向きのEncoderの中間ベクトルの加重平均\n",
    "        batch_size = h.data.shape[0] # ミニバッチのサイズを記憶\n",
    "        ws = [] # ウェイトを記録するためのリストの初期化\n",
    "        sum_w = Variable(xp.zeros((batch_size, 1), dtype='float32')) # ウェイトの合計値を計算するための値を初期化\n",
    "        # Encoderの中間ベクトルとDecoderの中間ベクトルを使ってウェイトの計算\n",
    "        for f, b in zip(fs, bs):\n",
    "            w = F.tanh(self.fh(f)+self.bh(b)+self.hh(h)) # 順向きEncoderの中間ベクトル、逆向きEncoderの中間ベクトル、Decoderの中間ベクトルを使ってウェイトの計算\n",
    "            w = F.exp(self.hw(w)) # softmax関数を使って正規化する\n",
    "            ws.append(w) # 計算したウェイトを記録\n",
    "            sum_w += w\n",
    "        # 出力する加重平均ベクトルの初期化\n",
    "        att_f = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        att_b = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        for f, b, w in zip(fs, bs, ws):\n",
    "            w /= sum_w # ウェイトの和が1になるように正規化\n",
    "            # ウェイト * Encoderの中間ベクトルを出力するベクトルに足していく\n",
    "            att_f += F.reshape(F.batch_matmul(f, w), (batch_size, self.hidden_size))\n",
    "            att_b += F.reshape(F.batch_matmul(b, w), (batch_size, self.hidden_size))\n",
    "        return att_f, att_b\n",
    "\n",
    "# Attention Sequence to Sequence Modelクラス\n",
    "class AttSeq2Seq(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, batch_col_size):\n",
    "        # Attention + Seq2Seqのインスタンス化\n",
    "        # :param vocab_size: 語彙数のサイズ\n",
    "        # :param embed_size: 単語ベクトルのサイズ\n",
    "        # :param hidden_size: 隠れ層のサイズ\n",
    "        super(AttSeq2Seq, self).__init__(\n",
    "            f_encoder = LSTMEncoder(vocab_size, embed_size, hidden_size), # 順向きのEncoder\n",
    "            b_encoder = LSTMEncoder(vocab_size, embed_size, hidden_size), # 逆向きのEncoder\n",
    "            attention = Attention(hidden_size), # Attention Model\n",
    "            decoder = AttLSTMDecoder(vocab_size, embed_size, hidden_size) # Decoder\n",
    "        )\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.decode_max_size = batch_col_size # デコードはEOSが出力されれば終了する、出力されない場合の最大出力語彙数\n",
    "        # 順向きのEncoderの中間ベクトル、逆向きのEncoderの中間ベクトルを保存するためのリストを初期化\n",
    "        self.fs = []\n",
    "        self.bs = []\n",
    "    \n",
    "    def encode(self, words, batch_size):\n",
    "        # Encoderの計算\n",
    "        # :param words: 入力で使用する単語記録されたリスト\n",
    "        # :param batch_size: ミニバッチのサイズ\n",
    "        # :return:\n",
    "        # 内部メモリ、中間ベクトルの初期化\n",
    "        c = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        h = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        # 順向きのEncoderの計算\n",
    "        for w in words:\n",
    "            c, h = self.f_encoder(w, c, h)\n",
    "            self.fs.append(h) # 計算された中間ベクトルを記録\n",
    "        # 内部メモリ、中間ベクトルの初期化\n",
    "        c = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        h = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))        \n",
    "        # 逆向きのEncoderの計算\n",
    "        for w in reversed(words):\n",
    "            c, h = self.b_encoder(w, c, h)\n",
    "            self.bs.insert(0, h) # 計算された中間ベクトルを記録\n",
    "        # 内部メモリ、中間ベクトルの初期化\n",
    "        self.c = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))\n",
    "        self.h = Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))        \n",
    "\n",
    "    def decode(self, w):\n",
    "        # Decoderの計算\n",
    "        # :param w: Decoderで入力する単語\n",
    "        # :return: 予測単語\n",
    "        att_f, att_b = self.attention(self.fs, self.bs, self.h)\n",
    "        t, self.c, self.h = self.decoder(w, self.c, self.h, att_f, att_b)\n",
    "        return t\n",
    "\n",
    "    def reset(self):\n",
    "        # インスタンス変数を初期化する\n",
    "        # Encoderの中間ベクトルを記録するリストの初期化\n",
    "        self.fs = []\n",
    "        self.bs = []\n",
    "        # 勾配の初期化\n",
    "        self.zerograds()\n",
    "        \n",
    "    def __call__(self, enc_words, dec_words=None, train=True):\n",
    "        # 順伝播の計算を行う関数\n",
    "        # :param enc_words: 発話文の単語を記録したリスト\n",
    "        # :param dec_words: 応答文の単語を記録したリスト\n",
    "        # :param train: 学習か予測か\n",
    "        # :return: 計算した損失の合計 or 予測したデコード文字列\n",
    "        enc_words = enc_words.T\n",
    "        if train:\n",
    "            dec_words = dec_words.T\n",
    "        batch_size = len(enc_words[0]) # バッチサイズを記録\n",
    "        self.reset() # model内に保存されている勾配をリセット\n",
    "        enc_words = [Variable(xp.array(row, dtype='int32')) for row in enc_words] # 発話リスト内の単語をVariable型に変更\n",
    "        self.encode(enc_words, batch_size) # エンコードの計算\n",
    "        t = Variable(xp.array([0 for _ in range(batch_size)], dtype='int32')) # <eos>をデコーダーに読み込ませる\n",
    "        loss = Variable(xp.zeros((), dtype='float32')) # 損失の初期化\n",
    "        ys = [] # デコーダーが生成する単語を記録するリスト\n",
    "        # デコーダーの計算\n",
    "        if train: # 学習の場合は損失を計算する\n",
    "            for w in dec_words:\n",
    "                y = self.decode(t) # 1単語ずつをデコードする\n",
    "                t = Variable(xp.array(w, dtype='int32')) # 正解単語をVariable型に変換\n",
    "                loss += F.softmax_cross_entropy(y, t) # 正解単語と予測単語を照らし合わせて損失を計算\n",
    "            return loss\n",
    "        else: # 予測の場合はデコード文字列を生成する\n",
    "            for i in range(self.decode_max_size):\n",
    "                y = self.decode(t)\n",
    "                y = xp.argmax(y.data) # 確率で出力されたままなので、確率が高い予測単語を取得する\n",
    "                ys.append(y)\n",
    "                t = Variable(xp.array([y], dtype='int32'))\n",
    "                if y == 0: # EOSを出力したならばデコードを終了する\n",
    "                    break\n",
    "            return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "\n",
    "id_to_char = {}\n",
    "char_to_id = {}\n",
    "\n",
    "\n",
    "def _update_vocab(txt):\n",
    "    chars = list(txt)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            tmp_id = len(char_to_id)\n",
    "            char_to_id[char] = tmp_id\n",
    "            id_to_char[tmp_id] = char\n",
    "\n",
    "\n",
    "def sequence_load_data(file_name='addition.txt', seed=1984):\n",
    "    file_path = file_name\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('No file: %s' % (file_name) )\n",
    "        return None\n",
    "\n",
    "    questions, answers = [], []\n",
    "\n",
    "    for line in open(file_path, 'r'):\n",
    "        idx = line.find('_')\n",
    "        questions.append(line[:idx])\n",
    "        answers.append(line[idx:-1])\n",
    "\n",
    "    # create vocab dict\n",
    "    for i in range(len(questions)):\n",
    "        q, a = questions[i], answers[i]\n",
    "        _update_vocab(q)\n",
    "        _update_vocab(a)\n",
    "\n",
    "    # create numpy array\n",
    "    x = numpy.zeros((len(questions), len(questions[0])), dtype=numpy.int)\n",
    "    t = numpy.zeros((len(questions), len(answers[0])), dtype=numpy.int)\n",
    "\n",
    "    for i, sentence in enumerate(questions):\n",
    "        x[i] = [char_to_id[c] for c in list(sentence)]\n",
    "    for i, sentence in enumerate(answers):\n",
    "        t[i] = [char_to_id[c] for c in list(sentence)]\n",
    "\n",
    "    # shuffle\n",
    "    indices = numpy.arange(len(x))\n",
    "    if seed is not None:\n",
    "        numpy.random.seed(seed)\n",
    "    numpy.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    t = t[indices]\n",
    "\n",
    "    # 10% for validation set\n",
    "    split_at = len(x) - len(x) // 10\n",
    "    (x_train, x_val) = x[:split_at], x[split_at:]\n",
    "    (t_train, t_val) = t[:split_at], t[split_at:]\n",
    "\n",
    "    return (x_train, t_train), (x_val, t_val)\n",
    "\n",
    "\n",
    "def get_vocab():\n",
    "    return char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_val, t_val) = sequence_load_data('addition.txt')\n",
    "char_to_id, id_to_char = get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_val.shape, t_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = x_train[:, ::-1], x_val[:, ::-1]\n",
    "char_to_id, id_to_char = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1',\n",
       " 1: '6',\n",
       " 2: '+',\n",
       " 3: '7',\n",
       " 4: '5',\n",
       " 5: ' ',\n",
       " 6: '_',\n",
       " 7: '9',\n",
       " 8: '2',\n",
       " 9: '0',\n",
       " 10: '3',\n",
       " 11: '8',\n",
       " 12: '4'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 学習\n",
    "\n",
    "# 教師データ\n",
    "data = [\n",
    "    [[\"初めまして。\"], [\"初めまして。よろしくお願いします。\"]],\n",
    "    [[\"どこから来たんですか？\"], [\"日本から来ました。\"]],\n",
    "    [[\"日本のどこに住んでるんですか？\"], [\"東京に住んでいます。\"]],\n",
    "    [[\"仕事は何してますか？\"], [\"私は会社員です。\"]],\n",
    "    [[\"お会いできて嬉しかったです。\"], [\"私もです！\"]],\n",
    "    [[\"おはよう。\"], [\"おはようございます。\"]],\n",
    "    [[\"いつも何時に起きますか？\"], [\"6時に起きます。\"]],\n",
    "    [[\"朝食は何を食べますか？\"], [\"たいていトーストと卵を食べます。\"]],\n",
    "    [[\"朝食は毎日食べますか？\"], [\"たまに朝食を抜くことがあります。\"]],\n",
    "    [[\"野菜をたくさん取っていますか？\"], [\"毎日野菜を取るようにしています。\"]],\n",
    "    [[\"週末は何をしていますか？\"], [\"友達と会っていることが多いです。\"]],\n",
    "    [[\"どこに行くのが好き？\"], [\"私たちは渋谷に行くのが好きです。\"]]\n",
    "]\n",
    "\n",
    "# 定数\n",
    "EMBED_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 6 # ミニバッチ学習のバッチサイズ数\n",
    "BATCH_COL_SIZE = 15\n",
    "EPOCH_NUM = 50 # エポック数\n",
    "#N = len(data) # 教師データの数\n",
    "#N = len(x_train)\n",
    "N = 100\n",
    "\n",
    "# 教師データの読み込み\n",
    "data_converter = DataConverter(batch_col_size=BATCH_COL_SIZE) # データコンバーター\n",
    "data_converter.load(data) # 教師データ読み込み\n",
    "vocab_size = len(data_converter.vocab) # 単語数\n",
    "\n",
    "# モデルの宣言\n",
    "model = AttSeq2Seq(vocab_size=vocab_size, embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, batch_col_size=BATCH_COL_SIZE)\n",
    "# ネットワークファイルの読み込み\n",
    "#network = \"./att_seq2seq_network/*******************network\"\n",
    "#serializers.load_npz(network, model)\n",
    "opt = optimizers.Adam()\n",
    "opt.setup(model)\n",
    "opt.add_hook(optimizer.GradientClipping(5))\n",
    "if FLAG_GPU:\n",
    "    model.to_gpu(0)\n",
    "model.reset()\n",
    "\n",
    "# 学習開始\n",
    "print(\"Train\")\n",
    "\n",
    "def train(epoch_num=10):\n",
    "    st = datetime.datetime.now()\n",
    "    for epoch in range(epoch_num):\n",
    "        # ミニバッチ学習\n",
    "        perm = np.random.permutation(N) # ランダムな整数列リストを取得\n",
    "        total_loss = 0\n",
    "        max_iters = N // BATCH_SIZE \n",
    "        for i in range(max_iters):\n",
    "            #enc_words = data_converter.train_queries[perm[i:i+BATCH_SIZE]]\n",
    "            #dec_words = data_converter.train_responses[perm[i:i+BATCH_SIZE]]\n",
    "            enc_words = x_train[perm[i : i+BATCH_SIZE]]\n",
    "            dec_words = t_train[perm[i : i+BATCH_SIZE]]\n",
    "\n",
    "            model.reset()\n",
    "            loss = model(enc_words=enc_words, dec_words=dec_words, train=True)\n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            total_loss += loss.data\n",
    "            opt.update()\n",
    "        #output_path = \"./att_seq2seq_network/{}_{}.network\".format(epoch+1, total_loss)\n",
    "        #serializers.save_npz(output_path, model)\n",
    "        if (epoch+1)%10 == 0:\n",
    "            ed = datetime.datetime.now()\n",
    "            print(\"epoch:\\t{}\\ttotal loss:\\t{}\\ttime:\\t{}\".format(epoch+1, total_loss, ed-st))\n",
    "            st = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predict\n",
      "初めまして。 => ['員', 'ん', 'は', 'た', 'ござい', 'ござい', 'ござい', 'は', '友達', 'お', 'た', 'の', '野菜', '6', '会っ']\n",
      "どこから来たんですか？ => ['員', 'ん', '！', 'まし', '会い', '会い', '何', 'し', 'どこ', 'どこ', '会社', 'と', 'ます', 'ます', 'た']\n",
      "日本のどこに住んでるんですか？ => ['員', 'ん', '！', '！', '時', '員', 'どこ', '時', '員', 'どこ', '員', '行く', '取っ', '員', 'たくさん']\n",
      "仕事は何してますか？ => ['員', 'ん', '！', 'まし', '員', 'よう', '時', 'も', 'でき', 'いる', '朝食', 'から', 'が', '会っ', 'た']\n",
      "お会いできて嬉しかったです。 => ['？', '？', 'お', 'が', 'た', 'と', 'よう', 'よう', 'よう', 'が', '嬉しかっ', '会い', '会い', '会い', '会い']\n",
      "おはよう。 => ['員', 'ん', 'どこ', 'を', 'は', 'を', 'は', 'を', 'ござい', '週末', 'か', '来', 'まし', '会い', '会い']\n",
      "いつも何時に起きますか？ => ['員', 'ん', '！', 'まし', '員', 'よう', '時', 'も', '嬉しかっ', 'まし', 'が', 'が', '取る', 'と', 'よう']\n",
      "朝食は何を食べますか？ => ['員', 'ん', '！', 'まし', '員', 'よう', '時', 'に', 'から', 'も', 'まし', 'まし', 'が', 'が', 'が']\n",
      "朝食は毎日食べますか？ => ['員', 'ん', '！', 'まし', '員', '時', 'も', 'でき', 'いる', '朝食', 'から', 'が', '会っ', 'た', 'の']\n",
      "野菜をたくさん取っていますか？ => ['員', 'ん', '！', '時', '！', '時', '員', 'どこ', '時', '員', '会社', '会っ', '会っ', 'も', 'でき']\n",
      "週末は何をしていますか？ => ['員', 'ん', '！', '時', 'に', 'から', 'まして', 'まし', 'まし', '員', 'よう', '時', 'も', 'でき', 'いる']\n",
      "どこに行くのが好き？ => ['員', 'ん', '！', '時', '員', 'どこ', '員', '行く', '時', '員', 'どこ', '員', 'どこ', '員', '行く']\n"
     ]
    }
   ],
   "source": [
    "# 予測\n",
    "\n",
    "print(\"\\nPredict\")\n",
    "def predict(model, query):\n",
    "    enc_query = data_converter.sentence2ids(query, train=False)\n",
    "    dec_response = model(enc_words=enc_query, train=False)\n",
    "    response = data_converter.ids2words(dec_response)\n",
    "    print(query, \"=>\", response)\n",
    "\n",
    "predict(model, \"初めまして。\")\n",
    "predict(model, \"どこから来たんですか？\")\n",
    "predict(model, \"日本のどこに住んでるんですか？\")\n",
    "predict(model, \"仕事は何してますか？\")\n",
    "predict(model, \"お会いできて嬉しかったです。\")\n",
    "predict(model, \"おはよう。\")\n",
    "predict(model, \"いつも何時に起きますか？\")\n",
    "predict(model, \"朝食は何を食べますか？\")\n",
    "predict(model, \"朝食は毎日食べますか？\")\n",
    "predict(model, \"野菜をたくさん取っていますか？\")\n",
    "predict(model, \"週末は何をしていますか？\")\n",
    "predict(model, \"どこに行くのが好き？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq2seq(model, question, correct, id_to_char, verbose=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 頭の区切り文字\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model(enc_words=question, train=False)\n",
    "\n",
    "    # 文字列へ変換\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbose:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "        if correct == guess:\n",
    "            print(colors['ok'] + '☑' + colors['close'] + ' ' + guess)\n",
    "        else:\n",
    "            print(colors['fail'] + '☒' + colors['close'] + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "id_to_char = {}\n",
    "char_to_id = {}\n",
    "\n",
    "\n",
    "def _update_vocab(txt):\n",
    "    chars = list(txt)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            tmp_id = len(char_to_id)\n",
    "            char_to_id[char] = tmp_id\n",
    "            id_to_char[tmp_id] = char\n",
    "\n",
    "\n",
    "def sequence_load_data(file_name='addition.txt', seed=1984):\n",
    "    file_path = file_name\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('No file: %s' % (file_name) )\n",
    "        return None\n",
    "\n",
    "    questions, answers = [], []\n",
    "\n",
    "    for line in open(file_path, 'r'):\n",
    "        idx = line.find('_')\n",
    "        questions.append(line[:idx])\n",
    "        answers.append(line[idx:-1])\n",
    "\n",
    "    # create vocab dict\n",
    "    for i in range(len(questions)):\n",
    "        q, a = questions[i], answers[i]\n",
    "        _update_vocab(q)\n",
    "        _update_vocab(a)\n",
    "\n",
    "    # create numpy array\n",
    "    x = numpy.zeros((len(questions), len(questions[0])), dtype=numpy.int)\n",
    "    t = numpy.zeros((len(questions), len(answers[0])), dtype=numpy.int)\n",
    "\n",
    "    for i, sentence in enumerate(questions):\n",
    "        x[i] = [char_to_id[c] for c in list(sentence)]\n",
    "    for i, sentence in enumerate(answers):\n",
    "        t[i] = [char_to_id[c] for c in list(sentence)]\n",
    "\n",
    "    # shuffle\n",
    "    indices = numpy.arange(len(x))\n",
    "    if seed is not None:\n",
    "        numpy.random.seed(seed)\n",
    "    numpy.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    t = t[indices]\n",
    "\n",
    "    # 10% for validation set\n",
    "    split_at = len(x) - len(x) // 10\n",
    "    (x_train, x_val) = x[:split_at], x[split_at:]\n",
    "    (t_train, t_val) = t[:split_at], t[split_at:]\n",
    "\n",
    "    return (x_train, t_train), (x_val, t_val)\n",
    "\n",
    "\n",
    "def get_vocab():\n",
    "    return char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t10\ttotal loss:\t107.73482\ttime:\t0:00:45.905651\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-63-ab685e56fad5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-ab685e56fad5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "break\n",
    "acc_list = []\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    train(10)\n",
    "    continue\n",
    "    #trainer.fit(x_train, t_train, max_epoch = 1,\n",
    "    #           batch_size = batch_size, max_grad = max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_val)):\n",
    "        question, correct = x_val[[i]], t_val[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_val)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
